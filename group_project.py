# -*- coding: utf-8 -*-
"""group_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fH91Gjz5ct5HbvnAwuxrbdVwYmeJy7AH
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer


# %matplotlib inline
sns.set_style("whitegrid")

data=pd.read_csv("/content/creditcard.csv")
df=pd.DataFrame(data)
df.head()

data_info = data.info()

pd.set_option("display.float", "{:.2f}".format)
data.describe()

df.shape

df.loc[0:6]

df.describe()

x=df['Amount'].mean()
round(x,2)

df['Amount']. isnull()

df['Amount']. isnull().sum()

df['Class']. isnull()

duplicate_records = data.duplicated().sum()

missing_data = data.isnull().sum()

df['Class']. isnull().sum()

data_description = data.describe()

data_info, missing_data, duplicate_records, data_description

LABELS = ["True", "Fraud"]

count_classes = pd.value_counts(data['Class'], sort = True)
count_classes.plot(kind = 'bar', rot=0)
plt.title("Transaction Class Distribution")
plt.xticks(range(2), LABELS)
plt.xlabel("Class")
plt.ylabel("Frequency");

corr=df[['Time','V3','V4','Amount','Class']].corr()
corr

df['V3']
df[['Time','V3','V4','Amount','Class']]

from matplotlib import pyplot as plt
data['Amount'].plot(kind='line', figsize=(8, 4), title='Amount')
plt.gca().spines[['top', 'right']].set_visible(False)

import pandas as pd
from matplotlib import pyplot as plt
data['Time'].plot(kind='hist', bins=20, title='Time')
plt.gca().spines[['top', 'right',]].set_visible(False)

data.columns

data.Class.value_counts()

cheating= data[data['Class']==1]
normal = data[data['Class']==0]
pd.concat([cheating.Amount.describe(), normal.Amount.describe()], axis=1)

pd.concat([cheating.Time.describe(), normal.Time.describe()], axis=1)

df.hist(column="Time", figsize=(7,6), bins=20)

LABELS = ["Normal", "Fraud"]

count_classes = pd.value_counts(data['Class'], sort = True)
count_classes.plot(kind = 'bar', rot=0)
plt.title("Transaction Class Distribution")
plt.xticks(range(2), LABELS)
plt.xlabel("Class")
plt.ylabel("Frequency");

df.hist(figsize=(10,20), bins=20)

"""# Data Mining

Handle Missing Values
"""

imputer = SimpleImputer(strategy='median')
data_imputed = imputer.fit_transform(data)

"""Convert back to DataFrame for easier handling"""

data_imputed = pd.DataFrame(data_imputed, columns=data.columns)
print("Data after handling missing values:")
print(data_imputed.head())

"""**Binning**

Binning by mean
"""

data_imputed['Binned_Amount'] = pd.cut(data_imputed['Amount'], bins=5, labels=False)

"""Group by the binned values and calculate the mean"""

binned_means = data_imputed.groupby('Binned_Amount')['Amount'].mean()

"""Map the means back to the Binned_Amount column"""

data_imputed['Binned_Amount'] = data_imputed['Binned_Amount'].map(binned_means)
#Output

print("Data after binning:")
print(data_imputed[['Amount', 'Binned_Amount']].head())

"""# **Reduction**

Dimensionality Reduction
"""

from sklearn.decomposition import PCA

# Dimensionality Reduction using PCA
pca = PCA(n_components=10)
data_reduced = pca.fit_transform(data_imputed)

# Convert back to DataFrame for easier handling
data_reduced = pd.DataFrame(data_reduced, columns=[f'PC{i}' for i in range(1, 11)])
print("Data after PCA:")
print(data_reduced.head())

"""Attribute Subset Selection"""

from sklearn.ensemble import RandomForestClassifier

"""Feature selection using Random Forest"""

model = RandomForestClassifier()
model.fit(data_imputed.drop(columns=['Class']), data_imputed['Class'])
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

"""Select top 10 features"""

top_features = indices[:10]
data_selected = data_imputed.iloc[:, top_features]

"""Convert back to DataFrame for easier handling"""

data_selected = pd.DataFrame(data_selected, columns=data.columns[top_features])
print("Data after feature selection:")
print(data_selected.head())

"""# **Transformation**

**Normalization**
"""

from sklearn.preprocessing import MinMaxScaler

"""*Normalizing*"""

scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(data_selected)

"""*Convert back to DataFrame for easier handling*"""

data_normalized = pd.DataFrame(data_normalized, columns=data_selected.columns)
print("Data after normalization:")
print(data_normalized.head())

"""**Feature Engineering**"""

data['New_Feature'] = data['Amount'] * data['Time']#Creating a new feature
print("Data with new feature:")
print(data[['Amount', 'Time', 'New_Feature']].head())



"""# **Data Mining**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

"""Initialize model"""

rf_model = RandomForestClassifier()
lr_model = LogisticRegression(max_iter=1000)
svc_model = SVC(probability=True)

"""Train and evaluate RandomForestClassifier"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix


print(df.columns)


df = df.dropna(subset=['Class'])



X = df[['V1', 'V2', 'V3']]


y = df['Class']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_y_pred = rf_model.predict(X_test)
print("RandomForestClassifier Results:")
print("Classification Report:")
print(classification_report(y_test, rf_y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, rf_y_pred))

"""Train and evaluate Logistic Regression"""

lr_model.fit(X_train, y_train)
lr_y_pred = lr_model.predict(X_test)
print("\nLogistic Regression Results:")
print("Classification Report:")
print(classification_report(y_test, lr_y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, lr_y_pred))

"""Train and evaluate Support Vector Classifier"""

svc_model.fit(X_train, y_train)
svc_y_pred = svc_model.predict(X_test)
print("\nSupport Vector Classifier Results:")
print("Classification Report:")
print(classification_report(y_test, svc_y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, svc_y_pred))

"""Evaluation of model"""

from sklearn.model_selection import cross_val_score

"""Cross-validation for RandomForestClassifier"""

rf_cv_scores = cross_val_score(rf_model, X, y, cv=5)
print("\nRandomForestClassifier Cross-validation Scores:", rf_cv_scores)
print("Mean Cross-validation Score:", rf_cv_scores.mean())

"""Cross-validation for Logistic Regression"""

lr_cv_scores = cross_val_score(lr_model, X, y, cv=5)
print("\nLogistic Regression Cross-validation Scores:", lr_cv_scores)
print("Mean Cross-validation Score:", lr_cv_scores.mean())

"""Cross-validation for Support Vector Classifier"""

svc_cv_scores = cross_val_score(svc_model, X, y, cv=5)
print("\nSupport Vector Classifier Cross-validation Scores:", svc_cv_scores)
print("Mean Cross-validation Score:", svc_cv_scores.mean())

from sklearn.model_selection import GridSearchCV

"""Define parameter grids"""

rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30]
}

lr_param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear']
}

svc_param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

"""Grid search for RandomForestClassifier"""

rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=5, n_jobs=-1)
rf_grid_search.fit(X_train, y_train)
print("\nBest parameters for RandomForestClassifier:", rf_grid_search.best_params_)

"""Grid search for Logistic Regression"""

lr_grid_search = GridSearchCV(estimator=lr_model, param_grid=lr_param_grid, cv=5, n_jobs=-1)
lr_grid_search.fit(X_train, y_train)
print("\nBest parameters for Logistic Regression:", lr_grid_search.best_params_)

"""Grid search for Support Vector Classifier"""

svc_grid_search = GridSearchCV(estimator=svc_model, param_grid=svc_param_grid, cv=5, n_jobs=-1)
svc_grid_search.fit(X_train, y_train)
print("\nBest parameters for Support Vector Classifier:", svc_grid_search.best_params_)



"""### **Deployement**"""

! pip install streamlit -q

! wget -q -O - ipv4.icanhazip.com

! streamlit run group_project.ipynb & npx localtunnel --port 8501